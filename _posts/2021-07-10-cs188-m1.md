---
layout: category-post
title:  "CS 188 MT"
date:   2021-07-10
categories: notes
permalink: "cs188-mt"
---

## Introduction

These notes are for [CS 188: Introduction to Artificial Intelligence](https://inst.eecs.berkeley.edu/~cs188/su21/) taught by Mesut Yang and Carl Qi in Summer 2021. They cover the topics up to the midterm, except probability. If you're in the class, good luck on the exam!

## Searches

Planning agents search future states by simulating the world. Reflex agents choose an action based on the world's current state. A **search problem** consists of a state space, a successor function (with actions and costs), and a start state/goal test(s). A **solution** transforms the start state to the goal state. A search state is **intentionally** smaller than the world state.

A **state space graph** includes nodes as world configurations with arcs representing successors; it's usually *too big* to build in memory. A **search tree** represents plans and their outcomes with the start state as the roto node; we usually can't build the whole tree. The **fringe** of a search tree denotes the partial plans under consideration.

Generally, a tree search maintains a fringe and uses a **strategy** to select a node for expansion. A search algorithm is **complete** if it is guaranteed to find a solution and **optimal** if it finds the least cost path. For a given search tree, we notate $$b$$ as the branching factor and $$m$$ as the maximum depth. Types of searches:

- **Depth first search** 
  - expands deepest node first, fringe is LIFO
  - complete if there aren't cycles, but not optimal
  - time complexity $$O(b^m)$$ and space complexity $$O(bm)$$
- **Breadth first search**
  - expend shallowest node first, fringe is FIFO
  - complete and optimal if costs are the same
  - time complexity $$O(b^s)$$ and space complexity $$O(b^s)$$, where $$s$$ is the depth of the shallowest solution
- **Iterative deepening**
  - repeatedly use DFS with incrementing depth limit
  - "best of both worlds"
- **Uniform cost search**
  - expand cheapest node first (backward cost), fringe is priority queue
  - complete and optimal with finite costs and positive arc costs
  - time and space complexity $$O(B^{C^*/\epsilon})$$, where the solution costs $$C^*$$ and arcs cost at least $$\epsilon$$
- **Greedy search**
  - estimates distance to solution (forward cost) and expands the node that seems closest
  - complete but not optimal
  - worst case is like a bad DFS
- **A* search**
  - expands nodes with the lowest total backward and forward cost
  - requires an **admissible** (consistent) heuristic $$h$$  where $$0 \leq h(n) \leq h^*(n)$$ where $$h^*$$ is the true cost, often derived from relax problems
  - there's a tradeoff between the quality of the heuristic's estimate and work per node
  - a heuristic $$h_a$$ dominates $$h_c$$ if $$\forall n, h_a(n) \geq h_c(n)$$; the trivial heuristic always gives 0
  - complete and optimal, faster than BFS since it expands "toward" the goal

A tree search that fails to detect repeated states can cause exponentially more work. **Graph search** keeps track of a set of expanded states (closed set) and skips nodes that have already been visited. A* graph search must be consistent, which means $$h(A) - h(A) \leq \text{cost}(A\text{ to }C)$$. Consequently, the forward cost along a path never decreases.

## Game Trees

**Adversarial games** are deterministic games with two or more players that are zero sum and include perfect information. A **strategy** recommends a move from the current state. An example formulation could include states $$S$$, players $$P$$, actions $$A$$, a transition function $$S \times A \to S$$, a terminal test $$S -\to {t, f}$$, and terminal utilities $$S \times P \to R$$. A solution is a **policy**. Whereas a **general game** involves independent utilities for agents, a zero-sum game can be represented with a single value.

The **value** of a state is the best achievable utility from the state. In **minimax**, players alternate turns and each player computes the best utility against an optimal adversary. Minimax *may not be optimal* against a non-optimal adversary that makes mistakes. Just like DFS, it has time complexity $$O(b^m)$$ and space complexity $$O(bm)$$.

**Alpha-beta pruning** prunes nodes that would never be picked. Suppose we are MAX and we are computing the MIN-VALUE at some node $$n$$. Let $$\alpha$$ be the best value that MAX can get at any choice along the current path to the root. If $$n$$'s value becomes worse than $$\alpha$$, then MAX will avoid it and stop considering the other children. A symmetric logic holds for the MAX version and $$\beta$$.

**Depth-limited search** computes terminal utilities with an **evaluation function**, an approximation of the minimax value is usually a weighted linear sum of features. It's much more realistic, but there's no guarantee of optimal play. With insufficient depth and a bad evaluation function, we could have re-planning agents that thrash around.

Actions could have uncertain outcomes because of unseen variables, unpredictable opponents, and failure to act. **Expectimax search** computes the average score under optimal play with chance nodes instead of min nodes. Having a probabilistic belief about another agent does *not* necessarily mean that the other agent is actually acting randomly.

## Reinforcement Learning

A **Markov Decision Process** (MDP) is defined by a set of states $$S$$, a set of actions $$A$$, a transition function $$T(s, a, s')$$, a reward function $$R(s, a, s')$$, a start state, and potentially a terminal state. One way to solve them is expectimax. For sequences of utilities, we can model a preference for utility *sooner* by exponentially discounting future utility by a discount factor $$gamma$. If a game last forever, we could consider a finite horizon, use discounting, or provide an absorbing state.

Define the value of a state $$V^*(s)$$ as the expected utility starting in $$s$$ and acting optimally, the value of a q-state $$Q^*(s, a)$$ as the expected utility after having taken an action from a state $$s$$ and acting optimally, and the optimal policy $$\pi^*(s)$$ as the optimal action from $$s$$. The expectimax value of the state is:
$$
V^*(s) = \max_a Q^*(s, a) = \max_a T(S, a, s') [R(s, a, s') + \gamma V^*(s')]
$$
We can perform **value iteration** to update the values using the Bellman equation until convergence, with complexity $$O(S^2A)$$. For a fixed policy $$\pi$$, we can compute the utility of a state $$s$$ as:
$$
V^\pi (s) = \sum_{s'} T(s, \pi(s), s') [R(s, \pi(s), s') + \gamma V^\pi(s')]
$$
The efficiency for an update would be $$O(s^2)$$, but we could also just solve it as a system of linear equations. To extract a policy from values, we can evaluate:
$$
\pi^*(s) = \arg \max_a \sum_{s'} T(s, a, s') [R(s,  a, s') + \gamma V^\pi(s')]
$$
Value iteration is slow, with the a max that rare changes and a policy that converges far before the values do. **Policy iteration** involves performing **policy evaluation** and updating the policy using **policy extraction**.

These are all examples of **offline planning**, where quantities are determined through computation. **Reinforcement learning** involves receiving feedback in the form of rewards and learning to maximize expected rewards - we *don't know* the transition function or the reward function.

In **model based learning**, we use repeated episodes to estimate the transition function and determine the rewards. An alternative is **model-free learning**. In **passive reinforcement learning**, we perform **direct evaluation** by averaging the sum of discounting rewards from each state across samples. **Temporal difference learning** places greater weights on recent samples:
$$
V^\pi (s)  \leftarrow  (1-\alpha) V^\pi(s) + \alpha [R(s, \pi(s), s') + \gamma V^\pi(s')]
$$


But we can't extract a policy from TD value learning. To update our policy, we need **Q-learning**, which uses a similar update function:
$$
Q(s, a)  \leftarrow  (1 - \alpha) Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a')]
$$


Surprisingly, Q-learning converges to the optimal policy *even if* you're acting suboptimally, a process caleld **off-policy learning**. To find the optimal policy, the agent has to balance exploration and exploitation. We could have the agent occasionally take random moves, but a better approach is to reward unvisited states with estimated value $$u$$ and visit count $$n$$ with an optimistic utility $$f(u, n) = u + k/n$$.

**Regret** measure total mistake cost, the difference between expected rewards and optimal rewards. Minimizing regret requires optimally learning to be optimal. In the real world, we'd like to generalize across similar states using **feature-based representation**. The q function is expressed using weights:
$$
Q(s, a) + w_1 f_1(s, a) + w_2 f_2(s, a) + ... + w_nf_n(s, a)
$$


With linear Q-functions, updates adjust the weights of active features:
$$
w_i \leftarrow w_i + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] f_i(s, a)
$$


Approximation works because it moves in the direction of the gradient that minimizes error. If we had only one point $$x$$ and features $$f(x)$$. a target value $$y$$ and weights $$w$$, then notice that taking the derivative of least-squared error justifies the linear Q-function update:
$$
e(w) = \frac{1}{2}(y - \sum_k w_kf_k(x))^2\\
\frac{\partial e(w)}{\partial w_m} = -(y - \sum_k w_k f_k(x)) f_m(x)
$$


Approximate Q-learning lets us generalize and avoid **overfitting**. Even if it doesn't model Q-values well, it can still be effective at maximizing rewards. **Policy search** involves starting with an ok solution and fine-tuning with hill-climbing. To effectively hill-climb, methods can exploit lookahead structure, sample wisely, and change multiple parameters at a time.

