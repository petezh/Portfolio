---
layout: category-post
title:  "Stat 134 F"
date:   2021-08-11
categories: notes
permalink: "stat134-f
---

## Introduction

I took these notes for [Stat 134: Concepts of Probability](https://bcourses.berkeley.edu/courses/1506088) taught by Professor Ibser. It is based on [*Probability* by Jim Pitman](resources/stat134/pitman.pdf).

## Continuous Distribution

A probability density curve is denoted $$f(x)$$. Probabilities are defined by areas under the graph:
$$
P(a \leq X \leq b) = \int^b_a f(x) dx
$$
Expectation and variance are defined as expected:
$$
E(X) = \int^\infty_{-\infty} xf(x) dx, Var(X) = E(X^2) - E(X)^2
$$

Two continuous variables $$X$$ and $$Y$$ are independent if, for intervals $$A$$ and $$B$$,
$$
P(X \in A, Y \in B) = P(X \in A)P(Y \in B)
$$
A random variable has a uniform distribution on $$(a, b)$$ if $$f(x)$$ is constant on $$(a, b)$$ and $$0$$ elsewhere.

The probability density function for a standard normal variable $$Z$$ is:
$$
\phi(z) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} z^2}
$$
 A variable $$X$$ has standard deviation $$\sigma$$ and mean $$\mu$$ if $$X = \mu + \sigma Z$$.

If a distribution is approximate by a density $$f(x)$$, then the average of a function $$g(x)$$ over the $$n$$ values is approximated by:
$$
\frac{1}{n} \sum_{i=1}^n \approx \int_{-\infty}^{\infty} g(x)f(x) dx
$$
A random time $$T$$ has **exponential distribution** with rate $$\lambda$$ if $$f(t) = \lambda e^{-\lambda t}$$. For the exponential survival function, $$E(T) = SD(T) = \frac{1}{\lambda}$$. The distribution has the **memoryless property**:
$$
P(T > t + s | T > t) = P(T > s)
$$
Here, $\lambda$$ is roughly the death rate. In the context of half lives $$h$$, we can compute $$\lambda = \frac{\log{2}}{h}$$.

If $$T_r$$ is the time of the $$r$$th arrival after time $$0$$ n a Poisson process, then $$T_r$$ is modeled by a **gamma** distribution defined by:
$$
P(T_r > t) = \sum_{k=0}^{r-1} e^{-\lambda t} \frac{(\lambda t)^k}{k!}
$$
and probability density
$$
f_{r, \lambda} (t) = [\tau(r)^-1 \lambda ^r t^{4-1} e^{-\lambda t}] \text{ for } t\geq 0 \text{ where } \tau(r) = \int^\infty_0 t^{r-1}e^{-t} dt
$$

A **change of variable** from $$X$$ to $$Y$$ involves a function $$y = g(x)$$ with nonzero derivative. For a linear function $$y = ax+b$$, the function shrinks the length of every interval by $$|a|$$.
$$
f_{aX+b}(y) = \frac{1}{|a|}f_X(\frac{y-b}{a})
$$
For a one-to-one change of variable $$y = g(x)$$, the density is:
$$
f_Y(y) = f_X(x)/|\frac{dy}{dx}|, y=g(x)
$$
If $$X$$ has the same distribution as $$Y$$, then so do $$g(X)$$ and $$g(Y)$$.

A cumulative distribution function computes $$F(x) = P(X \leq x)$$. For the uniform distribution, the density is $$F(X)$$ on $$(0, 1)$$. CDFs make it easy to compute the distributions of maximum $$X_\max = \max(X_1, ..., X_n)$$ and minimum $$X_\min = \min(X_1, ..., X_n)$$. The CDF of the maximum is $$F_\max (x) = F_1(x) F_2(x) ... F_n(x)$$. For positive random varaibles, $$E(X) = \int^\infty_0 (1-F_L(x))dx$$. We can invert the distribution function to yield $$x = F^{-1}(p)$$. For any CDF $$F$$ with an inverse, $$F^{-1}(U)$$ has CDF $$F$$.

The kth order statistic is denoted $$X_{(k)}$$ The density is given by the probability that one of the $$X$$'s is equal to some $$x$$ times the probability that exactly $$k-1$$ of the others are less than $$k$$:
$$
f_{(k)}(x) = nf(x){n-1 \choose k-1} (F(x))^{k-1} (1-F(x))^{n-k}
$$
The Beta distribution is defined by the density:
$$
\frac{1}{B(r, s)} x^{r-1} (1-x)^{s-1} \text{ with } B(r, s) = \int_0^1 x^{r-1} (1-x)^{s-1} dx = \frac{(r-1)!(s-1)!}{(r+s-1)!}
$$
We also have $$E(X) = \frac{r}{r+s}$$.

## Continuous Joint Distribution

The **joint distribution** for a pair of RVs $$X$$ and $$Y$$ is defined by $$P(B) = P((X, Y) \in B)$$ where $$B$$ is a subset of over the plane. If $$X$$ and $$Y$$ are uniform, independent RVs, then $$(X, Y)$$ is uniformly distributed on a rectangle. Uniform distributions can extend into an arbitrary number of dimensions with $$n$$ independent random variables $$U_1, ..., U_n$$.

The join probability density function $f(x, y)$ behaves similarly to the one-dimension density function:
$$
P((X, Y) \in B) = \int_{ B}\int f(x, y) dx dy
$$
The infinitesimal probability is:
$$
P(X \in dx, Y \ in dy) = f(x, y) dx dy
$$
The nonnegative and total integral 1 constraints continue to apply. For independent RVs $$X$$ and $$Y$$, the joint density is $$f(x, y) =f_X(x) f_Y(y)$$. The expectation of a function of $$(X, Y)$$ is:
$$
E(g(X, Y)) = \int\int g(x, y)f(x, y) dx dy
$$

## Independent Normal  Variables

For two normal, independent variables $$X$$ and $$Y$$ with the same normal distribution. the joint density is:
$$
f(x, y) = \phi(x)\phi(y) = c^2 e^{-\frac{1}{2} (x^2 + y^2)}
$$
If we consider the random variable $$R = X^2 + Y^2$$, we find that:
$$
f_R(r) = 2\pi rc^2e^{-\frac{1}{2}r^2}
$$
The integral must be 1, which lets us calculate $$c = 1/\sqrt{2\pi}$$. We can also use $$S = E(X^2) = \frac{1}{2}E(R^2)$$ to compute
$$
f_S(s) = f_R(r) / \frac{ds}{dr} = \frac{1}{2}e^{-\frac{1}{2}s}, E(S) = 2
$$
If $$X$$ and $$Y$$ are normal distributions with parameters $$(\lambda, \sigma^2)$$ and $$(\mu, \gamma^2)$$, then $$X+Y$$ has normal $$(\lambda + \mu, \sigma^2 + \gamma^2)$$ distribution.

We derive the chi-square distribution with $$n$$ degrees of freedom and apply skewness correction as follows:
$$
f_{R^2_n}(t) = (2^{n/2} \tau(n/2))^{-1}t^{(n/2)-1}e^{-t/2}, P(R_n^2 \leq x) \approx \Phi(z) - \frac{\sqrt{2}}{3\sqrt{n}} (z^2 - 1)\phi(z), z = (x-n)/\sqrt{2n}
$$

## Operations

In the discrete case for the sum of RVs is:
$$
P(X + Y = z) \sum_x P(X = x, Y = z-x)
$$
We can reformulate these in terms of densities, where the second equation holds for independent RVs:
$$
f_{X+Y}(z) = \int^\infty_{-\infty} f(x, z-x)dx = \int^\infty_{-\infty} f_X(x) f_Y(z-x)dx
$$
![](resources/stat134/sums.PNG)

The density of the sum of $$n$$ independent uniform variables rapidly becomes normal as $$n$$ increases, The distribution of the ratio follows:
$$
f_{Y/X} (z) = \int^{\infty}_{-\infty} |x| f(x, xz) dx
$$

## Dependence

Dependence between two variables can be understand as the marginal distribution of $$X$$ and conditional distribution of $$Y$$. The distribution of $$Y$$ can be computed as a weighted average while the condition distribution of $$X$$ given $$Y$$ can be found via Bayes rule.
$$
P(Y=y) = \sum_x P(Y=y|X = x)P(X = x), P(X = x, Y = y) = \frac{P(X=x, Y=y)}{P(Y=y)}
$$
The expectation of $$Y$$ given an event $$A$$ is:
$$
E(Y | A) = \sum_y yP(Y = y | A)
$$
Some properties:
$$
E(X + Y | A) = E(X | A) + E(Y | A)
$$

$$
E(Y) = \sum_x E(Y | X=x) P(X = x)
$$

$$
E(Y) = E[E(Y | X)]
$$

With infinitesimals:
$$
P(A | X = x) = \frac{P(A, X \in dx)}{P(X \in dx)}, P(A) = \int P(A | X=x) f_X(x) dx
$$
The conditional density is calculated as:
$$
f_Y(y | X=x) = f(x, y)/f_X(x), f(x, y) = f_X(x) f_Y(y | X=x)
$$

$$
E[g(Y)] = \int E[g(Y) | X =x] f_X(x) dx 
$$

## Covariance and Correlation

Covariance is defined as $$Cov(X, Y) = E[(X -\mu_X)(Y - \mu_Y)] = E(XY) - E(X) E(Y)$$. For indicators $$E(I_A) = P(A)$$ and $$E(I_B) = P(B)$$, we have $$Cov(I_A, I_B) = P(AB) - P(A)P(B)$$. Notice that the sign of covariance indicates the direction of dependence. Correlation is defined as $$Corr(X, Y) = \frac{Cov(X, Y)}{SD(X)SD(Y)}$$. Two variables are uncorrelated if $$Corr(X, Y) = Cov(X, Y) = 0$$ or $$E(XY) = E(X)E(Y)$$. The variance of the sum of $$n$$ variables is:
$$
Var(\sum_k X_k) = \sum_k Var(X_k) + 2\sum_{j<k} Cov(X_j, X_k)
$$
Covirance is bilinear:
$$
Cov(\sum_i a_i X_i, \sum_j b_jY_j) = \sum_i \sum_j a_i b_j Cov(X_i, Y_j)
$$
$$X$$ and $$Y$$ have standard bivariate normal distribution if $$Y = pX + \sqrt{1-p^2} Z$$ where $$Z$$ is an independent standard normal variable. Given $$X=x$$, $$Y = normal(px + 1-p^2)$$, and vice versa. For independent normal variables $$Z_i$$, the joint distribution of $$V = \sum_i a_iZ_i$$ and $$W = \sum_i b_iZ_i$$ is bivariate normal and independent only if they are uncorrelated.







